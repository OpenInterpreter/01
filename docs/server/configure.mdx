---
title: "Configure"
description: "Configure your 01 instance"
---

A core part of the 01 server is the interpreter which is an instance of Open Interpreter.
Open Interpreter is highly configurable and only requires updating or creating a profile.
Properties such as `model`, `context_window`, and many more can be updated here.

To open the directory of all profiles, run:

```bash
# View profiles
poetry run 01 --profiles
```

To apply a profile to your 01 instance, use the `--profile` flag followed by the name of the profile:

```bash
# Use profile
poetry run 01 --profile <profile_name>
```

### Standard Profiles

`default.py` is the default profile that is used when no profile is specified. The default TTS is OpenAI.

`fast.py` uses elevenlabs and groq, which are the fastest providers.

`local.py` uses coqui TTS and runs the --local explorer from Open Interpreter.

### Custom Profiles

If you want to make your own file, you can do so by creating a new file in the `profiles` directory.
The easiest way is to duplicate an existing profile and then update values as needed. Be sure to save the profile with a unique name.

Remember to add `interpreter.tts = ` to set the text-to-speech provider.

To use a custom profile, run:

```bash
# Use custom profile
poetry run 01 --profile <profile_name>
```

### Example Profile

````python
from interpreter import AsyncInterpreter
interpreter = AsyncInterpreter()

# This is an Open Interpreter compatible profile.
# Visit https://01.openinterpreter.com/profile for all options.

# 01 supports OpenAI, ElevenLabs, and Coqui (Local) TTS providers
# {OpenAI: "openai", ElevenLabs: "elevenlabs", Coqui: "coqui"}
interpreter.tts = "openai"

# Connect your 01 to a language model
interpreter.llm.model = "gpt-4o"
interpreter.llm.context_window = 100000
interpreter.llm.max_tokens = 4096
# interpreter.llm.api_key = "<your_openai_api_key_here>"

# Tell your 01 where to find and save skills
interpreter.computer.skills.path = "./skills"

# Extra settings
interpreter.computer.import_computer_api = True
interpreter.computer.import_skills = True
interpreter.computer.run("python", "computer")  # This will trigger those imports
interpreter.auto_run = True
interpreter.loop = True
interpreter.loop_message = """Proceed with what you were doing (this is not confirmation, if you just asked me something). You CAN run code on my machine. If you want to run code, start your message with "```"! If the entire task is done, say exactly 'The task is done.' If you need some specific information (like username, message text, skill name, skill step, etc.) say EXACTLY 'Please provide more information.' If it's impossible, say 'The task is impossible.' (If I haven't provided a task, say exactly 'Let me know what you'd like to do next.') Otherwise keep going. CRITICAL: REMEMBER TO FOLLOW ALL PREVIOUS INSTRUCTIONS. If I'm teaching you something, remember to run the related `computer.skills.new_skill` function."""
interpreter.loop_breakers = [
    "The task is done.",
    "The task is impossible.",
    "Let me know what you'd like to do next.",
    "Please provide more information.",
]

# Set the identity and personality of your 01
interpreter.system_message = """

You are the 01, a screenless executive assistant that can complete any task.
When you execute code, it will be executed on the user's machine. The user has given you full and complete permission to execute any code necessary to complete the task.
Run any code to achieve the goal, and if at first you don't succeed, try again and again.
You can install new packages.
Be concise. Your messages are being read aloud to the user. DO NOT MAKE PLANS. RUN CODE QUICKLY.
Try to spread complex tasks over multiple code blocks. Don't try to complex tasks in one go.
Manually summarize text."""
````

### Hosted LLMs

The default LLM for 01 is GPT-4-Turbo. You can find this in the default profile in `software/source/server/profiles/default.py`.

The fast profile uses Llama3-8b served by Groq. You can find this in the fast profile in `software/source/server/profiles/fast.py`.

```python
# Set your profile with a hosted LLM
interpreter.llm.model = "gpt-4o"
```

### Local LLMs

You can use local models to power 01.

Using the local profile launches the Local Explorer where you can select your inference provider and model. The default options include Llamafile, Jan, Ollama, and LM Studio.

```python
# Set your profile with a local LLM
interpreter.llm.model = "ollama/codestral"

# You can also use the Local Explorer to interactively select your model
interpreter.local_setup()
```

### Hosted TTS

01 supports OpenAI and Elevenlabs for hosted TTS.

```python
# Set your profile with a hosted TTS service
interpreter.tts = "elevenlabs"
```

### Local TTS

For local TTS, Coqui is used.

```python
# Set your profile with a local TTS service
interpreter.tts = "coqui"
```

<Note>
  When using the Livekit server, the interpreter.tts setting in your profile
  will be ignored. The Livekit server currently only works with Deepgram for
  speech recognition and Eleven Labs for text-to-speech. We are working on
  introducing all-local functionality for the Livekit server as soon as
  possible.
</Note>

### Deploy and Configure Voice Feature in GCP VM

To deploy and configure the voice feature in a GCP VM where Open Interpreter is already installed, you can use the provided `deploy_voice_feature.sh` script. This script automates the entire process, including installing necessary dependencies, setting up environment variables, and starting the server with the voice feature enabled.

1. Download the `deploy_voice_feature.sh` script to your GCP VM.
2. Make the script executable:
   ```bash
   chmod +x deploy_voice_feature.sh
   ```
3. Run the script:
   ```bash
   ./deploy_voice_feature.sh
   ```

The script will handle the installation of dependencies, configuration of environment variables, and starting the server with the voice feature enabled using the OpenAI Realtime Voice API.
